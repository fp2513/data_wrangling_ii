---
title: "101024 reading_data_from_web"
output: github_document
date: "2024-10-10"
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(httr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

Scraping data ( copy and paste data can be a pain)l rvest package
Selector gadget; clicking around in a webpage and it will highlight
rvest pacakage (download the html, then extract all the elements of the html with the selector CSS selector)
Rather than scraping, API (a way to request specific data from server)

```{r}
url = "https://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm"

drug_use_html = read_html(url)
```

Read the html and now i need to get the data that i actually need from it 
Get the pieces that i actually need

```{r}
drug_use_html %>% 
  html_table()
```

I want to extract a table that the html already has, html_table can identify the table, finds all 15 tables on the page 

To take the first element out of the table 

```{r}
marj_use_df = 
drug_use_html %>% 
  html_table() %>% 
  first()
```

There is an issue with the data we imported; first row (there is this NOTE)
Slice function allows to select rows by number

```{r}
marj_use_df = 
drug_use_html %>% 
  html_table() %>% 
  first() %>% 
  slice(-1)
```

Learning assessment:

```{r}
nyc_cost_df = 
  read_html("https://www.bestplaces.net/cost_of_living/city/new_york/new_york") %>% 
  html_table(header = TRUE) %>% 
  first()
```

First row is the header so need to change that with (header = TRUE)

If there are multiple tables how to tell it which you want? (first function takes the first table, nth function from ?first can tell you the nth of the list)

## CSS selector

Want to create a dataframe with movie time, how long it is, and metascore of starwars movies (table in the html that is not in a table that we want it into a dataframe)
Here CSS selector will help to recognise what we want on the page 

Import all the html, then extract the elements that we want

```{r}
swm_url = "https://www.imdb.com/list/ls070150896/"

swm_html = read_html(swm_url)
```

html element needs to define CSS

```{r}
swm_url = "https://www.imdb.com/list/ls070150896/"

swm_html = read_html(swm_url)

title_vec = 
  swm_html %>% 
  html_elements(".ipc-title-link-wrapper .ipc-title__text") %>% 
  html_text()

metascore_vec = 
  swm_html %>% 
  html_elements(".metacritic-score-box") %>% 
  html_text()

runtime_vec = 
  swm_html %>% 
  html_elements(".dli-title-metadata-item:nth-child(2)") %>% 
  html_text()

swm_df = 
  tibble(
    title = title_vec,
    score = metascore_vec,
    runtime = runtime_vec)

```

Careful that reading the html once, so then just doing 'swm_html %>% '. Each time read_html you are reloading and visiting the page, there are sometimes issues with over-visiting a page

CSS tags are unstable if the html changes (since it is a unique tag)

## Using API to get data from web 

Someone has set up a sever and can send request to server and they will send it back to me (csv format)

(to actions, API, CSV format, and copy the API format)

```{r}
nyc_water = 
  GET("https://data.cityofnewyork.us/resource/ia2d-e54m.csv") %>% 
  content()
```

GET function is a common API call to get the data

content() gives you just the content of the table (wihtout content () command will also get excess information of the data; content type, size 1.47kB)

```{r}
brfss_smart2010 = 
  GET("https://data.cityofnewyork.us/resource/ia2d-e54m.csv", 
      query = list ("$limit" = 5000)) %>% 
  content()
```

The query is for this specific API this is how they are telling us to exveed 1000 rows of the data, how we make request is API specific 

Pokemon API

```{r}
pokemon =
  GET("https://pokeapi.co/api/v2/pokemon/ditto") %>% 
  content()
```

Sometimes the data that comes out of the API is not very clean (datasets that are more complicated, importing data but need to convert it to the dataframe that we need it to be)



